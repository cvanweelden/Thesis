\section{Metric alignment}

%absolute distance constraints

%cost function

%loss function

%minimization


\section*{Old section}

%What is the problem with other metric learning methods?
%input to metric learning methods are equivalence or ranking constraints, qualitative similarity function that establish relation between pairs or triples of points, metric learning does not take advantage of continuous distance measure, discrete metric learning, have to set arbitrary threshold, ranking learning, easier to learn from a restricted sample if you have more specific information, have to generate a lot of training data, mostly just interested in brining similar points together, don't have any specific target distance function, what if you do have samples from target distance, how can you leverage this?

In existing metric learning methods the input is a set of equivalence constraints or ranking constraints. Equivalence and ranking constraints are useful when we are interested in a qualitative similarity function, such as when points are assigned to a single class and we want the distance between same-class points to be small. But, in order to apply metric learning to problems where we are interested in a real-valued similarity function, we will have to generate equivalence or ranking constraints. Ranking constraints can be generated by sampling points and sorting them according to their similarity value, while equivalence constraints can be generated by setting a threshold on the similarity value. This approach introduces extra parameters and ignores the exact similarity values, thereby throwing away useful information.

%How does our method solve this problem? What is the idea behind our method?
%metric alignment learns from real-valued distances, sampled from target distance, idea is to make Euclidean distance in feature space predictive of target distance, assumes that target metric is also a valid pseudo metric in some target space which is a linear transformation of feature space, however if not then we can still find the best linear transformation, but stay linear to keep methods efficient, just a single transformation of the features and can still use the same methods for finding neighbors/clustering

With metric alignment we take a different approach. We do not ignore the exact values, but instead train on a set of distance constraints. Distance constraints are generated by sampling pairwise similarities and encoding them as real-valued distances. These distances are then used as target distance in a regression problem. The regression problem minimizes the difference between the target distances and the learned metric distance. Like other metric learning methods we parameterize our metric as a Mahalanobis metric:  \todo{I'll probably introduce Mahalanobis distance in section 2: "Metric learning background". Move this there.}
\begin{equation}
d_{\mat{M}}(\vec{x}_i, \vec{x}_j) = \sqrt{(\vec{x}_i-\vec{x}_j)^T \mat{M} (\vec{x}_i - \vec{x}_j)}.
\label{eq:mahalanobis}
\end{equation}
By substituting $\mat{M} = \mat{L}^T \mat{L}$ we can compute the Mahalanobis distance as Euclidean distance in a linearly transformed input space:
\begin{equation}
d_{\mat{L}}(\vec{x}_i, \vec{x}_j) =  ||\mat{L} (\vec{x}_i - \vec{x}_j)||_2.
\label{eq:mahalanobis_transformation}
\end{equation}
This allows us to apply the metric alignment method as a linear transformation on the input space in a pre-processing step, and thus to use efficient methods for approximate nearest neighbor search or clustering that have been developed for Euclidean space. %Furthermore, these linear approximations have already been proven to be powerful in metric learning for discrete classes and rankings.

%How well does it work?
%toy example set that is easy to visualize, show some images of before and after transformation, also for ranking and LMNN, or otherwise show example from real data that happens to be a clear example
\todo{Show an example of effect on a toy set, a la the squares and circles in Schultz and Joachims paper.}

\subsection{Learning from pairwise distance constraints}

%What are the details of the method?
%define target distance as some function which we can sample, minimize empirical risk over this training data, global vs local sampling, training data, pairs and real-valued distance, input as pair differences, minimize using gradient descent, assuming that it is easy to sample this is large scale learning because number of pairwise distances is exponential in number of points, we use SGD, Frobenius norm regularization, many parameters: exponential in number of features, hence regularization,

A pairwise distance constraint is a tuple $(\vec{x}_i, \vec{x}_j, d_{ij})$ consisting of a pair of points in the input space $\vec{x}_i, \vec{x}_j \in \mathcal{X}$ and their target distance $d_{ij} \in \mathbb{R}_0^+$. \todo{How to get these constraints, sampling globally vs locally.}

Given a training set $\mathcal{D}$ of pairwise distance constraints we want to learn a linear transformation $\mat{L}$ that makes distances in the transformed input space predictive of target distance. We define the loss between predicted distance and target distance as squared error, thus our empirical risk corresponds to the mean squared error over the training set:
\begin{equation}
\text{MSE}_\mathcal{D}(\mat{L}) = \frac{1}{|\mathcal{D}|}\sum_{(i,j) \in \mathcal{D}} \left ( d_{ij} - ||\mat{L} (\vec{x}_i - \vec{x}_j)||_2 \right)^2.
\label{eq:mse}
\end{equation}
We fit the parameters of the transformation matrix $\mat{L}$ to the training set by minimizing the empirical risk. Thus our output transformation is given by $\hat{\mat{L}} = \argmin_{\mat{L}} \text{MSE}_\mathcal{D}(\mat{L})$. 

\subsubsection{Optimization by stochastic gradient descent}

In order to fit the transformation matrix to the training set we use \acf{SGD}. Gradient descent is an iterative first order optimization algorithm that finds a local minimum of an error function. On each iteration it changes the parameters by a small step in the direction opposite to the gradient of the error function with respect to the parameters. Since the negative gradient is the direction of steepest descent, each step leads to a reduction in error. When the error function is defined as a sum of individual errors on examples in a training set, \ac{SGD} is a stochastic version of gradient descent where at each step the gradient is computed on the error of a single example. This does not require a summation over all examples in the training set for each update, which is beneficial when the training set is large. \todo{Highlight benefits of SGD. Discuss small-scale vs large scale learning, relevant papers of Bottou and Xu. Probably move this to background and have a SGD subsection. Should also discuss ASGD and convergence speed.}

In order to apply \ac{SGD} to our learning problem we define an error function which has the same minimum as (\ref{eq:mse}) but is easier to differentiate and optimize because of its form:
\begin{equation}
E_{\mathcal{D}}(\mat{L}) =  \sum_{(i,j) \in \mathcal{D}}  \frac{1}{4} \left ( d_{ij}^2 - ||\mat{L}(\vec{x}_i - \vec{x}_j)||_2^2 \right )^2.
\label{eq:opt_target}
\end{equation}
Here $\vec{x}_i - \vec{x}_j$ is the pairwise difference between two vectors in the input space $\mathcal{X}$ and $d_{ij}^2$ is the squared target distance for this pair of input vectors. $||\mat{L}(\vec{x}_i - \vec{x}_j)||_2^2$ is the squared Euclidean distance in the transformed space which is efficient to compute as the inner product of the transformed difference vector. The constant vector $\frac{1}{4}$ is there only for convenience in differentiation. 

Interpreting the optimization objective as $E_{\mathcal{D}} = \sum_{(i,j)\in\mathcal{D}} E_{ij}$ the corresponding gradient of the error $E_{ij}$ on a single pairwise distance constraint $(i,j) \in \mathcal{D}$ is given by:
\begin{equation}
\nabla E_{ij}(\mat{L})  = - \left ( d_{ij}^2 - ||\mat{L}(\vec{x}_i - \vec{x}_j)||_2^2 \right) \mat{L} \mat{C}_{ij},
\label{eq:opt_grad}
\end{equation}
where $\mat{C}_{ij} = (x_i - x_j)(x_i - x_j)^T$ is the outer product of the pairwise difference.

We initialize our transformation as the identity transformation $\mat{L} = \mat{I}$ and then update it at each iteration as:
\begin{equation}
\mat{L} \leftarrow \mat{L} - \eta_{t} \nabla E_{ij}(\mat{L}).
\label{eq:update}
\end{equation}
Here $t$ is the iteration number, $\nabla E_{ij}(\mat{L})$ is the gradient computed over the distance constraint for points $(i,j) \in \mathcal{D}$ and $\eta_{t}$ is the learning rate: a gain parameter which controls the step size. The learning rate should decrease and the schedule with which it decreases is important for the convergence speed of the optimization \cite{xu2011towards}. We use the parameterization proposed by \cite{xu2011towards} and calculate $\eta_{t}$ at each timestep $t$ as follows:
\begin{equation}
\eta_{t} = \frac{\eta_0}{\left(1+ a \eta_0 t \right)^c}
\label{eq:eta_update}
\end{equation}
For SGD without regularization we set $a = 1$ and we leave the parameter $c$ as a hyper parameter that needs to be tuned. Following \cite{bottou2008tradeoffs} we set $\eta_0$ by optimizing it on the first thousand points in the dataset and selecting the value that reduces loss the most.

\subsubsection{Relative error}

The squared error measure in (\ref{eq:mse}) assigns greater importance to larger pairwise differences. This might not be optimal for nearest neighbor applications since the points with smallest pairwise difference are most important for classification. \todo{Mention stress functions which have parameter for this.} In order to put equal weight on different length difference vectors, we define a squared relative error as loss function:
\begin{equation}
\text{MSRE}(\mat{L}) = \frac{1}{|\mathcal{D}|}\sum_{(i,j) \in \mathcal{D}} \left ( \frac{||\mat{L} (\vec{x}_i - \vec{x}_j)||}{d_{ij}} - 1 \right)^2.
\label{eq:msre}
\end{equation}
Analogous to (\ref{eq:opt_target}) we create an optimization objective $R_{\mathcal{D}}(\mat{L}) = \sum_{(i,j) \in \mathcal{D}} R_{ij}$ which leads to the following gradient $\nabla R_{ij}(\mat{L})$ for the squared relative error on the pairwise distance constraint $(i,j) \in \mathcal{D}$:\todo{Need to handle asymptote at $d_{ij} \rightarrow 0$.}
\begin{equation}
\nabla R_{ij}(\mat{L})  =  \left ( \frac{||\mat{L}(\vec{x}_i - \vec{x}_j)||_2^2}{d_{ij}^2} - 1 \right) \frac{1}{d_{ij}^2} \mat{L} \mat{C}_{ij},
\label{eq:relative_grad}
\end{equation}
In contrast to (\ref{eq:opt_grad}) this scales the residual error and gradient by the magnitude of the squared target distance $d_{ij}^2$. Using relative error leads to larger steps for updates computed on small distances, making nearest neighbors more important in the optimization.


\subsubsection{Regularization}

%learn on training set, distances are sampled, might contain noise, might not be entirely in line with real distribution, might contain sampling bias especially for local sampling, hence possibility for overfitting, many parameters ($D^2$ for a $D$-dimensional input space) 
The target distances in the training set need not be a perfect representation of the actual distribution of distances. The sampling process might be noisy and the samples might not be independent or identically distributed. Furthermore, the number of parameters that we fit is large: $D^2$ for a $D$-dimensional input space. This flexibility in combination with imperfect training data might lead to \emph{overfitting}. The optimization continues to decrease error on the training data, but error on unseen data (e.g. a validation set) stops decreasing and starts increasing as the learned transformation becomes more specific to the irregularities of the training data and less generally applicable.

One way to prevent overfitting is by early stopping, for example by monitoring the error on a separate validation set and stopping once this error no longer decreases. Another way that we describe here is by incorporating a regularization penalty into the objective function that we optimize. For this we define a cost function that penalizes large transformations:
\begin{equation}
C_{\mathcal{D}}(\mat{L}) =  E_{\mathcal{D}}(\mat{L}) + \lambda ||\mat{L}||_\textsc{f}^2.
\label{eq:structured_risk}
\end{equation}
Here $C_{\mathcal{D}}(\mat{L})$ is the cost function that we will minimize, $E_{\mathcal{D}}(\mat{L})$ is the error on the data, $ ||\mat{L}||_\textsc{f}^2$ signifying the Frobenius norm of $\mat{L}$ is the regularization penalty and $\lambda \in \mathbb{R}_0^+$ is a parameter that determines the strength of the regularization. The gradient for the update then becomes:
\begin{equation}
\nabla C_{ij}(\mat{L}) =  E_{ij}(\mat{L}) + \lambda 2\mat{L}.
\label{eq:structured_grad}
\end{equation}


\subsubsection{Mini-batch optimization}

\todo{This should also be in the SGD background section, making it just a parameter which we'll discuss further in the tuning section.}

\begin{equation}
\mat{L} \leftarrow \mat{L} - \eta_{t} \frac{1}{|\mathcal{B}_t|} \sum_{(i,j) \in \mathcal{B}_t} \nabla E_{ij}(\mat{L})
\label{eq:update_batch}
\end{equation}





%suppose a similarity measure which gives a real valued measure of similarity between two objects, inverted (dissimilarity measure) gives us a distance function or metric that we want to learn,
%uses training data in a way that makes the method applicable to problems characterized by real-valued distance functions, such as a structured loss function
%Applicable to problems characterized by real-valued distance functions, such as a structured loss function.
%We want distance to be inverse proportional to similarity. 
%Large margin approaches lead to dissimilar examples having sufficiently larger distance than similar examples, but does not enforce the more dissimilar examples to be further away than slightly dissimilar examples proportionally to their difference in dissimilarity. 
%Ranking constraint approaches do enforce this ranking, but doesn't enforce the proportionality.
%Metric alignment directly approximates given real-valued dissimilarity measure and therefore satisfies all these constraints.
%we would like examples with a larger loss to have greater distance.

%Metric alignment is posed as a simple convex optimization problem which can be solved without special purpose solvers.
%Solution can be written as an unconstrained convex optimization problem.

%training set: pairs of objects with given distance (not similarity since we learn a distance function)
%$\mathcal{X}_{\textsc{train}} \subseteq \mathcal{X}$
% training tuples $\mathcal{T} = \{(\vec{x}_i, \vec{x}_j, d_{ij})\}$ with input vectors $\vec{x}_i, \vec{x}_j \in \mathcal{X}_{\textsc{train}}$ and their pairwise distances $d_{ij} \in \mathbb{R}_{0}^{+}$.
%Suppose we have a problem in which we are interested in the similarity between input examples. These input examples are represented through a set of real-valued attributes, thus each example corresponds to a vector  $\vec{x} \in \mathbb{R}^n$ in the representation space. We now want to measure the similarity between two input vectors $\vec{x}_i$ and $\vec{x}_j$. Without any prior knowledge we could measure the Euclidean distance between 
% compare $d(\vec{x}_i, \vec{x}_j) = ||\vec{x}_i - \vec{x}_j||_2$
%learn a distance metric specific to the problem instead of relying on 'default' metrics (e.g. Euclidean), same data set and features but difffernt task
%learn M by minimizing SSE over this training set
%\begin{equation}
%M = \argmin_{M} \sum_{(\vec{x}_i, \vec{x}_j, d_{ij}) \in \mathcal{T}} \left(d_{ij} -  \sqrt{(\vec{x}_{i}-\vec{x}_{j})^{T} M (\vec{x}_{i}-\vec{x}_{j})}\right)^2
%\label{eq:min_sse_M}
%\end{equation}
%rewrite $M = L^T L$, learn L, M is always PSD and the metric thus a pseudo metric, L is a linear transformation of the feature space such that Euclidean norm of transformed difference vector (or difference between transformed vectors) corresponds to $d_M$
%\begin{equation}
%L = \argmin_{L} \sum_{(\vec{x}_i, \vec{x}_j, d_{ij}) \in \mathcal{T}} \left( d_{ij} -  ||L(\vec{x}_{i}-\vec{x}_{j})|| \right)^2
%\label{eq:min_sse_L}
%\end{equation}
%\begin{equation}
%L = \argmin_{L} \sum_{(\vec{x}_i, \vec{x}_j, d_{ij}) \in \mathcal{T}} \left( d_{ij}^2 - L\delta_{ij} \cdot L\delta_{ij} \right)^2
%\label{eq:min_sse_L}
%\end{equation}
