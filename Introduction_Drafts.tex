\section{Introduction (drafts)}



\subsection*{Exploratory draft 1.}

%1.1 machine learning features, feature space, measuring distance, finding similar samples, Euclidean distance
In machine learning problems samples are often represented by a set of attributes, also called features. Feature space. One thing that is often interesting is to measure the distances between samples based on these features, for example to find out which samples are the most alike by finding pairs of samples that are closest together. Suppose our samples are represented by vectors of real-valued feature values. These features then span a Euclidean space $\mathbb{R}^d$, thus we could use Euclidean distance to measure similarity between samples.

%1.2 use of metric in machine learning methods, kNN, k-means, SVM, effectiveness depends on metric
Several machine learning methods use a distance function at the core of their algorithm. For example, the \ac{kNN} algorithm classifies data samples by searching the training data for samples that are closest to the given sample in feature space and letting these neighbor samples vote on the class to assign to the given sample. Similarly, the $k$-means clustering algorithm clusters samples based on their distances in feature space. Support vector machines make use of a kernel function which represents squared Euclidean distances in a transformed feature space. All these methods depend on features being chosen such that distances in feature space are informative for the learning problem. This can be difficult to do however. Features are often chosen based on what attributes are available for measurement and handcrafting good features specific to each learning problem is time consuming.

%1.3 metric learning, parametric distance function, learn parameters, constraints based on class of samples
Metric learning is the problem of learning a distance function specific to a given problem given a feature space and a set of distance constraints as input. Instead of trying to hand-craft features, this approach aims to take features as given and learn a distance function that is most informative for the problem. This can be done by selecting a parametric distance function and then automatically learning the best parameters for given data samples and constraints. These constraints are based on the class of the data samples: samples of the same class should be close together while the distance between samples of different classes should be larger. 

%1.4 real-valued loss function instead of nominal values for classes, metric proportional to loss, predict loss from feature space by learning metric on real-valued constraints
There are other learning problems in which samples are not taken from disjunct classes, but in which there is a real-valued loss function. In this case, we would like our metric learning method to learn a distance function which is proportional to this loss. However, this loss is not measured in feature space, but in a ground-truth space. But for testing samples we do not have this ground truth, so we need to learn a distance function in the feature space which is aligned with the loss function in ground-truth space. Like in the $0/1$-loss case we can generate distance constraints from this real loss in order to train a parametric distance function. This thesis describes how this can be done and evaluates it on two datasets with real-valued loss functions.



\subsection*{Exploratory draft 2.}

%2.1 objects represented as vectors of feature values, feature space
In the field of machine learning, objects of interest are represented using a set of features. Features correspond to attributes of the object that can be encoded in some way. For example in the case of images, these features may simply be the pixel values, but they may also be more complex hand-crafted features computed from the basic pixel values. Each object is thus represented by a vector of feature values. We will refer to the space of all possible feature vectors as the feature space and in the case of $n$-dimensional real-valued vectors take this space to be $\mathbb{R}^n$.

%2.2 measure similarity by assuming small difference in feature vector means similar objects, used by kNN and k-means
Now suppose we would like to measure similarities between objects represented by feature values. We can make the assumption that similar objects result in similar vectors, and thus measure similarity by Euclidean distance in the feature space. This approach is the basis for the \ac{kNN} classification algorithm and the $k$-means clustering algorithm. 

%2.3 feature difference not always good measure for similarity, metric learning: learn distance function that is more useful
The assumption that similar objects result in similar feature vectors is not necessarily true however. In some cases our idea of similarity does not correspond directly to similarity in features, but depends on something which is not encoded in the features. In that case we can learn a distance function which is better suited to our idea of similarity. This approach is called metric learning. 

%2.4 train metric on class differences, we train on real-valued ground-truth metric, such as real-valued loss function in structured prediction problems
Most metric learning methods are trained on data from multi-class classification problems. That is, for each pair of objects they record whether the class is the same or different and generally, if the class is the same they constrain the distance to be small while if the classes differ the distance is constrained to be large. In contrast to these methods, we investigate whether we can make use of a real-valued ground-truth metric in order to learn a distance metric in feature space. This is useful in cases where the problem has a real-valued loss function which can be computed between data in the training set, but not at testing time when we have no access to the ground truth. Such a real-valued loss function is generally found in structured prediction tasks, where loss has to be measured between complex answers for which $0/1$-loss is uninformative.



\subsection*{Old introduction.}

%3.1 similarity between objects, distance function, applications (clustering, similarity search [product recommendation, document search], non-parametric methods based on distance or local density [kernel density estimation, k-nearest neighbor classification], visual identification)
Similarity between a pair of objects can be expressed as a distance function. A distance function maps representations of two objects to a positive real number. Choosing the correct distance function is important for many learning problems such as document search (finding documents most similar to the user query) or product recommendation (finding products bought by similar users or that are similar to the purchased product). It is also the defining factor in the effectiveness of unsupervised clustering techniques and non-parametric supervised learning algorithms such as kernel density estimation and k-nearest neighbor classification. This thesis is about automatically selecting the best distance function for a specific problem.

%3.2 representation of objects, feature space, distance in this space different from similarity, explain problem with clustering: picking different features or scaling will lead to different clusters which do not all correspond to our idea of similarity, common to use Euclidean or Hamming distance depending on the feature vector
A common approach %in the absence of prior knowledge
 is to represent objects as points in some high-dimensional feature space and to measure similarity by Euclidean distance in this space. This requires us to carefully choose a feature representation such that Euclidean distance corresponds to our notion of similarity. \todo{Evaluate  against feature selection?} However, in most cases the dataset and features are predetermined and the features  measure  basic attributes of the objects instead of being chosen specifically to satisfy a similarity measure. Furthermore, the similarity measure is specific to the task. For example, we would want to use a different similarity measure for face identification than for age verification even though both tasks have images of faces as input.

%3.3 metric learning: learn a distance function between object representations that corresponds to actual similarity, metric, parametric metrics, metric transform, Mahalanobis distance,  using discrete similarity for training or a similarity ranking,
A more advanced approach is to use metric learning to select a distance function. Metric learning algorithms use a parameterized distance function and automatically select the parameters that best correspond to a given notion of similarity. Learning the parameters requires a set of training data generated with the target metric. \todo{Define target metric.} However, most of these methods have been developed in the context of discrete classification problems with an unknown target metric. To still be able to learn a metric they use pairs of objects for which is given whether they belong to the same class or not as training data and learn a metric which best separates those classes. \todo{What does this solve and what does it not solve?}

%3.4 what if we have exact similarity scores represented as real numbers, but not discrete classes, e.g. structured problem, metric alignment, learn a metric that satisfies real similarity score
Because of the way such metric learning methods learn their parameters, they are not applicable to problems in which training examples cannot be assigned to a single class. For example if class inclusion is on a continuous scale or in the case of structured problems where the number of possible assignments is combinatorially large. However, in many of such problems there is a natural way of defining similarity between pairs of training example as a real number. \todo{This thesis presents ...}

%dimension reduction and visualization methods, (multi-dimensional scaling, isomap, LLE, SNE), mapping/embedding retaining pairwise distances, finding coordinates in some space which satisfy a similarity score
\todo{MDS etc}

%3.5 (bad) student network example
Student network example: suppose that we have a set of students and for each pair we record how much time those student spend interacting with each other, suppose further that there are two cliques in the class (jocks and nerds). Visualization techniques (such as MDS, Isomap, SNE) would be able to give us a map in which the distances between students would correspond to the interaction times. We could probably recognize the two groups in this map. But let's say a new student joins and we want to know where he will fit in this map. Since we don't have the interaction times, we will not be able to create a new map with him in it. Suppose then that we give each student a survey asking how much time they spend on which recreational activities. Metric learning methods can then learn a metric based on the survey answers that would allow us to predict the interaction distance between the new student and the old students. But these methods mostly require training data in which we specify which of the old students are in the same group. What if the teachers can't distinguish between the groups, or the groups are on a continuous spectrum where most students are only slightly more nerd than jock or visa versa? Metric alignment allows us to learn a metric on the survey results (like metric learning) while using the interaction time as training data (like visualization techniques).

%3.6 contributions (replace with those on paper)
contributions: 
1. describes and evaluates method using regression to learn a metric from exact similarity scores 
2. show that it improves performance of k-nearest neighbors on a real structured-prediction problem

%3.7 proposed method should be used when you need a distance function measuring similarity or a feature space in which Euclidean distances correspond to similarity between objects for which it is difficult to decide whether they belong to the same discrete class and the similarity you want to measure is known for at least a subset of the training set but not for new examples but training and test objects can be represented by the same set of features.
The proposed method should be used when you have some notion of similarity which is measurable for (at least a subset of) the training set, but not for unseen objects and you would like to have a feature space or distance function in which pairwise distances correspond to this notion of similarity. The similarity measure should be expressible as a distance $\in \mathbb{R}^{+}$ (positive real number or zero). If the precise similarity is not known but a discrete metric can be established (e.g. objects being labeled the same or different), then discrete metric learning can be used instead of the proposed method. If we know the pairwise similarity for all examples, but we do not have a representation for the objects, data visualization techniques such as \ac{MDS} can be used to find a n-dimensional mapping for the objects in which distances between objects correspond to pairwise similarity.



\subsection*{Start of old section 4.}

%4.1 metric alignment, continuous distance functions, unknown or unavailable during execution, transforms feature space, feature distances are predictive of training distance
The metric alignment method described in section \ref{metric_regression} is applicable to problems characterized by a continuous distance function. Given samples of pairwise distances, metric alignment learns a transformation of the feature space which makes Euclidean distances between feature vectors predictive of the sampled distance function. This is helpful when the exact formulation of the target distance function is unknown or unavailable during execution. For example, when samples of the target distance consists of pairwise similarities judged by a human expert who can not give you an exact formula for his judging process. Or another example, when the target distance is defined on the attributes that should be predicted. In this case we would be able to calculate exact distances during training, but not during execution.

%4.2 classification problems with continuous loss function, nearest neighbor methods, make distance predictive of loss for predicting neighbor label
A common class of such problems are structured prediction problems with continuous loss functions. Structured prediction problems are classification problems where the goal is not to predict some single output label for each example, but the goal is to predict some element of a structured output space $\vec{y} \in \mathcal{Y}$ where $\vec{y}$ is some complex element, for example a sequence or a tree of labels. Because of the complexity of the output elements, the 0-1 loss measure $L(\hat{y}, y) = \left[\hat{y} \ne y\right]$ that is commonly used in classification is no longer applicable since it treats a sequence which differs in only one element the same as a sequence which differs in all elements. Hence, such problems define a continuous loss function $L: \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}$, for example Hamming distance for sequences of labels.

%4.3 kNN for structured prediction, performs best when distance function is small if loss is small for k=1, have to predict loss from feature space
Now suppose we would like to apply \acf{kNN} classification to such a structured problem. In \ac{kNN} classification we predict an output by looking at the output corresponding to the $k$ closest training examples. Suppose we set $k = 1$ and we simply predict the output we have observed for the closest training example. This approach performs best when the closest example is the example with the lowest loss relative to the ground truth label. However, we do not have this loss available during execution to rank neighbors, thus `closest' is defined here as smallest distance in feature space. Hence, if we train our metric to be predictive of the pairwise loss, we might increase our performance.