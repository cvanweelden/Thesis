\section{Related work}

\quad


\subsection{Linear metric learning}

%this section highlights differences between metric learning and our own work, for survey see Kulis etc.
There is a large body of work in the metric learning literature, therefore this thesis does not aim to provide a complete overview. Instead, this section highlights the differences between the main metric learning approaches and the work presented in this thesis. Readers looking for a recent survey are directed to \cite{kulis2012metric}, which discusses a wide body of metric learning approaches with reference to their unified model for regularized transformation learning, which we discussed in Section \ref{sec:background_opimization}. Readers looking for a more complete discussion of specific methods are directed to \cite{bellet2013survey} and \cite{yang2006distance}.


\subsubsection{Similarity/dissimilarity constraints}

%metric learning methods, mostly based on classification, uses similarity/dissimilarity constraints which can not capture a target metric (important point to explain)
As discussed in the introduction, most metric learning methods have been formulated in the context of a classification problem. This context determines the type of background knowledge that is available for training the metric. Hence, most popular metric learning methods use similarity/dissimilarity constraints, with pairs of inputs being constrained to be similar if they share the same classification label as output, and dissimilar if their output label differs. \cite{davis2007information, kostinger2012large} Because these methods expect input pairs to be assigned to be either similar or dissimilar, they cannot be applied to a problem where similarity between pairs lies on a continuous scale. In comparison, the method presented in this thesis is specifically designed to align the learned metric to a target metric which can take any value. The method presented in this thesis can thus be applied to both binary similarity as well as continuous similarity.


\subsubsection{Relative distance constraints}

%metric learning methods with relative constraints, in theory able to encode target metric, would require a lot of triplets, not as applicable to large datasets, less strict supervision,
Metric learning methods that use relative distance constraints in the context of ranking problems, such as \cite{schultz2003learning}, are more similar to the work in this thesis since relative constraints can be used to indirectly encode a real-valued target metric. However, since these constraints do not directly encode the target distance they provide less supervision than the absolute distance constraints used by the method described in this thesis. Also, because of the large number of training triplets that would be needed to fully specify a target metric, these methods are less applicable to large datasets than the method presented in this thesis which relies on only a sampling of pairs.

%large margin methods, use relative constraints but in a discriminative fashion
Relative distance constraints are also used in the large-margin metric learning approach, as exemplified by \cite{weinberger2009distance, frome2007learning}, which has become popular because of good results on classification problems and its similarity to the well-known \ac{SVM} approach. Note however that because of the large-margin formulation these methods still depend on a binary similarity judgements for pairs of training points and are thus not applicable to problems with a real-valued target metric.


\subsubsection{Metric learning in structured prediction}

%metric learning for ranking \cite{mcfee2010metric}, learns distance function between query and train point, struct-SVM like solver that tries to satisfy as much of the training rankings as possible, satisfies specific rankings, not target metric (which would be between query and point)
By aligning the learned distance function to a real-valued loss function, the method described in this thesis is specifically suited for structured prediction problems which are characterized by such a loss function. In\cite{mcfee2010metric} metric learning is also applied in a structured prediction context and makes use of a real-valued loss function. The main differences with the approach described in this thesis is that McFee \& Lanckriet apply their method specifically to ranking problems and use a structural \ac{SVM} approach to learning. Since they specify their ranking problem as finding the best interleaving of relevant and irrelevant points, this method also requires binary similarity judgements. The loss function is only used to find the maximally violated constraints for the cutting-plane algorithm that they use for training. Therefore, this method is not applicable to learning from a target metric instead of from binary similarity judgements. \todo{Rewrite this paragraph since it is sooo messy.}


%metric learning methods for structured prediction \cite{guillaumin2009tagprop}, or attribute-based classification \cite{akata2013label}
%\cite{bellet2013survey} surveys several methods for dealing with structured input data, note that these methods deal with structured input data, while we deal with a regular feature space, but in combination with a target metric derived from structured output data with a continuous loss function, the surveyed metric learning methods still learn from similarity/dissimilarity pairs and are thus not applicable to a target metric


\subsubsection{Metric learning as regression}

%metric learning for regression \cite{weinberger2007metric} comes very close to what we do but is specific to the regression problem, \cite{lowe1995similarity} is very similar to that but learns a weighted Euclidean metric instead of full Mahalanobis


%\cite{meyer2011regression} formulates a regression method to learn a metric which they apply to similarity constraints and kernel learning, we use the same formulation but apply it to learning from a target metric



%\subsection{MDS and related techniques}

%MDS, supervised and uses a distance matrix to learn the new representation, used primarily for visualisation, transductive learning that does not generalize, while we operate in inductive paradigm and our method can be applied to unseen test points

%MDS stress functions significantly change the characteristics of the method, while we use only squared prediction error, other stress functions might also be included in our method

%sammon mapping, isomap, LLE, SNE



%\subsection{Representation learning}

%representation learning, unsupervised, focusses on properties of the learned representation such as meaningfulness, invariance, etc, but not on distances



%\subsection{Kernel learning}

%structured prediction methods with kernels generally use fixed kernels, in kernel learning either the parameters of a fixed kernel are learned or a kernel matrix in a transductive setting is learned, while we learn a full metric directly on the input space



%\subsection{Instance-based structured prediction}

%structured random forest \cite{kontschieder2011structured}, trees work as a nearest neighbor index, put examples close together if he have small entropy in labels, however uses distinct labels for entropy, while we use the target metric between the whole structured output which is also applicable to other structured problems that do not have distinct labels

%kernels for structured prediction SVM