\section{Related work}

\quad


\subsection{Linear metric learning}

%this section highlights differences between metric learning and our own work, for survey see Kulis etc.
There is a large body of work in the metric learning literature, therefore this thesis does not aim to provide a complete overview. Instead, this section highlights the differences between the main metric learning approaches and the work presented in this thesis. Readers looking for a recent survey are directed to \cite{kulis2012metric}, which discusses metric learning approaches with reference to a unified model for regularized transformation learning, a model which we discussed in Section \ref{sec:background_opimization}. Readers looking for a more complete discussion of specific methods are directed to \cite{bellet2013survey} and \cite{yang2006distance}.


\subsubsection{Similarity/dissimilarity constraints}

%metric learning methods, mostly based on classification, uses similarity/dissimilarity constraints which can not capture a target metric (important point to explain)
As discussed in the introduction, most metric learning methods have been formulated in the context of a classification problem. This context determines the type of background knowledge that is available for training the metric. Hence, most popular metric learning methods use similarity/dissimilarity constraints, with pairs of inputs being constrained to be similar if they share the same classification label as output, and dissimilar if their output label differs. \cite{davis2007information, guillaumin2009you, kostinger2012large} Because these methods expect input pairs to be assigned to be either similar or dissimilar, they cannot be applied to a problem where similarity between pairs lies on a continuous scale. In comparison, the method presented in this thesis is specifically designed to do just that, by fitting the learned metric to a target metric which can take any value. The method presented in this thesis can thus be applied to both binary similarity as well as continuous similarity.


\subsubsection{Relative distance constraints}

%metric learning methods with relative constraints, in theory able to encode target metric, would require a lot of triplets, not as applicable to large datasets, less strict supervision,
Metric learning methods that use relative distance constraints in the context of ranking problems, such as \cite{schultz2003learning}, are more similar to the work in this thesis because relative constraints can be used to indirectly encode a real-valued target metric. However, since these constraints do not directly encode the target distance they provide less supervision than the absolute distance constraints that are defined in this thesis. Also, because of the large number of training triplets that would be needed to fully specify a target metric, these methods are less applicable to large datasets than the method presented in this thesis which needs only a sampling of pairs.

%large margin methods, use relative constraints but in a discriminative fashion
Relative distance constraints are also used in the large-margin metric learning approach, as exemplified by \cite{weinberger2009distance, frome2007learning}, which has become popular because of good results on classification problems and its similarity to the well-known \ac{SVM} approach. Note however that because of the large-margin formulation these methods still depend on a binary similarity judgements for pairs of training points and are thus not applicable to problems with a real-valued target metric.


\subsubsection{Metric learning in structured prediction}

%metric learning for ranking \cite{mcfee2010metric}, learns distance function between query and train point, struct-SVM like solver that tries to satisfy as much of the training rankings as possible, satisfies specific rankings, not target metric (which would be between query and point)
Structured prediction problems are characterized by a real-valued loss function. The metric learning method described in this thesis can be applied to structured prediction problems by identifying the target metric with this loss function. In\cite{mcfee2010metric} metric learning is also applied to a structured prediction problem. The main differences with the approach described in this thesis is that McFee \& Lanckriet apply their method specifically to ranking problems and use a structural \ac{SVM} approach to learning. Since they specify their ranking problem as finding the best interleaving of relevant and irrelevant points, this method also requires binary similarity judgements. They do make use of the structured loss function in training the metric, but only to find maximally violated constraints for the cutting-plane algorithm that they use. Therefore, this method is not applicable to fitting a metric to a target metric such as is done by the method described in this thesis.

%metric learning methods for structured prediction \cite{guillaumin2009tagprop}
In \cite{guillaumin2009tagprop} metric learning is applied to the problem of image annotation, which can also be viewed as a structured problem. Like the work in this thesis, they use a nearest-neighbor approach to solving the problem. But instead of learning a distance function in feature space, they learn a weighted combination of base distance functions picked by hand. They optimize their weights by maximizing the log-likelihood for the training output, while this thesis takes the approach of directly optimizing the distance function instead of optimizing the resulting predictions.

%attribute-based classification \cite{akata2013label}
Attribute-based classification is one of the structured prediction problems investigated in this thesis. In \cite{akata2013label} label embedding is used as an approach to attribute-based classification. This can be viewed as learning a distance function between input features and attribute vectors. In comparison, this thesis presents a method that learns a distance function between pairs of input vectors. Thus, while this thesis takes a pure metric learning approach, \cite{akata2013label} takes a label embedding, or scoring function, approach.

%\cite{bellet2013survey} surveys several methods for dealing with structured input data, note that these methods deal with structured input data, while we deal with a regular feature space, but in combination with a target metric derived from structured output data with a continuous loss function, the surveyed metric learning methods still learn from similarity/dissimilarity pairs and are thus not applicable to a target metric
Note that, although \cite{bellet2013survey} investigates the application of metric learning to structured data, they focus on a structured input space, while the work in this thesis focuses on a structured output space. The methods discussed in \cite{bellet2013survey} rely on binary similarity/dissimilarity constraints for training.


\subsubsection{Metric learning as regression}

%\cite{lowe1995similarity} metric learning for kNN classification
The idea of approaching metric learning as a regression problem, as is done in this thesis, is not new. In \cite{lowe1995similarity} a weighted Euclidean metric is learned through conjugate gradient descent on a squared prediction error measure. Although this approach is similar in solution to the work in this thesis, the target is very different. Like most metric learning methods, this approach relies on the classification problem consisting of a set of discrete labels.

%metric learning for regression \cite{weinberger2007metric} comes very close to what we do but is specific to the regression problem,
The method in \cite{weinberger2007metric} learns a metric that minimizes regression error for a kernel regression algorithm. Since they formulate their error as the quadratic prediction error and minimize this using \ac{SGD}, they have a very similar derivation of the learning problem as the work presented in this thesis. However, Weinberger \$ Tesauro only apply their method directly to the kernel regression problem. Thus in their method the target metric is directly the function to be learned, while the method in this thesis uses the target metric to learn a better representation of the input features. 

%\cite{meyer2011regression} formulates a regression method to learn a metric which they apply to similarity constraints and kernel learning, we use the same formulation but apply it to learning from a target metric
The regression problem for metric learning from a squared prediction error is further investigated in \cite{meyer2011regression}. The work in this thesis uses exactly the same formulation for the \ac{SGD} problem as Meyer et al. and they further show fixed-rank and scale invariant variants, which are applicable as solving methods for the work in this thesis. However, in \cite{meyer2011regression} this method is only applied to a classical classification metric learning problem with similarity/dissimilarity constraints. The work in this thesis goes further than that and describes how this method can be applied to solve metric learning problems for structured prediction problems with absolute distance constraints.


\subsection{MDS and related techniques}

%MDS, sammon mapping, isomap, LLE, SNE supervised and uses a distance matrix to learn the new representation, used primarily for visualisation, transductive learning that does not generalize, while we operate in inductive paradigm and our method can be applied to unseen test points
Like the method in this thesis, \acf{MDS} and related techniques also use target distances as a training signal. This set of techniques includes \ac{MDS} \cite{venna2006local, chen2009local, chen2013stress}, kernel PCA \cite{scholkopf1997kernel}, isomap \cite{tenenbaum2000global}, and others. What they have in common is that they take as input a distance matrix, which is comparable with a target metric, since a target metric could also be specified as a distance matrix. However, all these methods learn an embedding of the training points in a lower-dimensional space. Thus instead of solving for the parameters of the distance function, they solve for the point coordinates. Because of this, these methods are very applicable to dimension reduction and visualization, but not to metric learning.

%MDS stress functions significantly change the characteristics of the method, while we use only squared prediction error, other stress functions might also be included in our method 
\cite{chen2013stress} discusses a family of optimization functions, called stress functions, for \ac{MDS} methods. The squared error function that this thesis uses corresponds to the most naive stress function and \cite{chen2013stress} show that by changing the stress function the characteristics of the solution can be tailored to a specific problem. Thus, finding a method to minimize these stress functions with respect to the metric parameters instead of to the point coordinates might change the characteristics of the method presented in this thesis and make it more broadly applicable.



\subsection{Representation learning}

%representation learning, unsupervised, focusses on properties of the learned representation such as meaningfulness, invariance, etc, but not on distances
The method presented in this thesis learns a feature transformation which makes aligns distances in feature space to a target metric, thus resulting in a feature representation which makes learning problems that are characterized by that target metric easier to solve. According to a recent survey, the problem of representation learning is ``learning representations of the data that make it easier to extract useful information when building classiﬁers or other predictors''. \cite{bengio2013representation} These descriptions seem very similar. However, the field of representation learning deals with unsupervised methods that learn a completely new representation that can be considered helpful in a general way, while this thesis presents a method that adapts a given representation in a supervised, task-specific way.



%\subsection{Kernel learning}

%structured prediction methods with kernels generally use fixed kernels, in kernel learning either the parameters of a fixed kernel are learned or a kernel matrix in a transductive setting is learned, while we learn a full metric directly on the input space



%\subsection{Instance-based structured prediction}

%structured random forest \cite{kontschieder2011structured}, trees work as a nearest neighbor index, put examples close together if he have small entropy in labels, however uses distinct labels for entropy, while we use the target metric between the whole structured output which is also applicable to other structured problems that do not have distinct labels

%kernels for structured prediction SVM