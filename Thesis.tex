\documentclass[a4paper,titlepage]{article}
\usepackage{listings}
\usepackage{graphicx}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{subcaption}
%\usepackage[scale=.65]{geometry}
%\usepackage{fullpage}
%\usepackage{a4wide}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{float}
\usepackage{acronym}
\usepackage{cite}
\usepackage{url}
\usepackage[usenames, pdftex]{color}
\usepackage{soul}
\usepackage{rotating}
\usepackage{booktabs}
\usepackage{todonotes}
\usepackage{amsfonts}
\usepackage{mdframed}
\usepackage{algorithmic}
\usepackage{algorithm}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\setlength{\marginparwidth}{4cm} %else todo notes will only fill halve the margin

\newcommand{\note}[1]{\marginpar{\colorbox{yellow}{\parbox{3.7cm}{#1}}}}

\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\mat}[1]{\mathbf{#1}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\acrodef{MDS}{multidimensional scaling}
\acrodef{MSE}{mean squared error}
\acrodef{LMNN}{large margin nearest neighbor}
\acrodef{kNN}[k-NN]{k-nearest neighbors}
\acrodef{HCRF}{hierarchical conditional random field}
\acrodef{SGD}{stochastic gradient descent}

%\title{Metric alignment}
%\title{Aligning metric to a distance function in an unknown space.}
%\title{Metric alignment: learning distance functions that linearly approximate arbitrary similarity measures.}
\title{Metric regression: linear approximation of arbitrary similarity measures.}
%\title{}
%\title{}
%\title{}
%\title{}


\author{Carsten van Weelden  \\ \texttt{cvanweelden@gmail.com} \\ 0518824 \and \small{Supervisor:} \\ Jan van Gemert \\ \texttt{J.C.vanGemert@uva.nl} \and University of Amsterdam\\
  The Netherlands \\ \\ In partial fulfillment etc.}
%\date{}                                           % Activate to display a given date or no date

\begin{document}
\maketitle

\begin{abstract}

\end{abstract}

\pagebreak




\tableofcontents

\pagebreak

















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% INTRODUCTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Introduction} 

%similarity between objects, distance function, applications (clustering, similarity search [product recommendation, document search], non-parametric methods based on distance or local density [kernel density estimation, k-nearest neighbor classification], visual identification)
Similarity between a pair of objects can be expressed as a distance function. A distance function maps representations of two objects to a positive real number. Choosing the correct distance function is important for many learning problems such as document search (finding documents most similar to the user query) or product recommendation (finding products bought by similar users or that are similar to the purchased product). It is also the defining factor in the effectiveness of unsupervised clustering techniques and non-parametric supervised learning algorithms such as kernel density estimation and k-nearest neighbor classification. This thesis is about automatically selecting the best distance function for a specific problem.

%representation of objects, feature space, distance in this space different from similarity, explain problem with clustering: picking different features or scaling will lead to different clusters which do not all correspond to our idea of similarity, common to use Euclidean or Hamming distance depending on the feature vector
A common approach %in the absence of prior knowledge
 is to represent objects as points in some high-dimensional feature space and to measure similarity by Euclidean distance in this space. This requires us to carefully choose a feature representation such that Euclidean distance corresponds to our notion of similarity. \todo{Evaluate  against feature selection?} However, in most cases the dataset and features are predetermined and the features  measure  basic attributes of the objects instead of being chosen specifically to satisfy a similarity measure. Furthermore, the similarity measure is specific to the task. For example, we would want to use a different similarity measure for face identification than for age verification even though both tasks have images of faces as input.

%metric learning: learn a distance function between object representations that corresponds to actual similarity, metric, parametric metrics, metric transform, Mahalanobis distance,  using discrete similarity for training or a similarity ranking,
A more advanced approach is to use metric learning to select a distance function. Metric learning algorithms use a parameterized distance function and automatically select the parameters that best correspond to a given notion of similarity. Learning the parameters requires a set of training data generated with the target metric. \todo{Define target metric.} However, most of these methods have been developed in the context of discrete classification problems with an unknown target metric. To still be able to learn a metric they use pairs of objects for which is given whether they belong to the same class or not as training data and learn a metric which best separates those classes. \todo{What does this solve and what doesn't it solve?}

%what if we have exact similarity scores represented as real numbers, but not discrete classes, e.g. structured problem, metric regression, learn a metric that satisfies real similarity score
Because of the way such metric learning methods learn their parameters, they are not applicable to problems in which training examples cannot be assigned to a single class. For example if class inclusion is on a continuous scale or in the case of structured problems where the number of possible assignments is combinatorially large. However, in many of such problems there is a natural way of defining similarity between pairs of training example as a real number. \todo{This thesis presents ...}

%dimension reduction and visualization methods, (multi-dimensional scaling, isomap, LLE, SNE), mapping/embedding retaining pairwise distances, finding coordinates in some space which satisfy a similarity score
\todo{MDS etc}


Student network example: suppose that we have a set of students and for each pair we record how much time those student spend interacting with each other, suppose further that there are two cliques in the class (jocks and nerds). Visualization techniques (such as MDS, Isomap, SNE) would be able to give us a map in which the distances between students would correspond to the interaction times. We could probably recognize the two groups in this map. But let's say a new student joins and we want to know where he will fit in this map. Since we don't have the interaction times, we will not be able to create a new map with him in it. Suppose then that we give each student a survey asking how much time they spend on which recreational activities. Metric learning methods can then learn a metric based on the survey answers that would allow us to predict the interaction distance between the new student and the old students. But these methods mostly require training data in which we specify which of the old students are in the same group. What if the teachers can't distinguish between the groups, or the groups are on a continuous spectrum where most students are only slightly more nerd than jock or visa versa? Metric regression allows us to learn a metric on the survey results (like metric learning) while using the interaction time as training data (like visualization techniques).

contributions: 
1. describes and evaluates method using regression to learn a metric from exact similarity scores 
2. show that it improves performance of k-nearest neighbors on a real structured-prediction problem

%proposed method should be used when you need a distance function measuring similarity or a feature space in which Euclidean distances correspond to similarity between objects for which it is difficult to decide whether they belong to the same discrete class and the similarity you want to measure is known for at least a subset of the training set but not for new examples but training and test objects can be represented by the same set of features.
The proposed method should be used when you have some notion of similarity which is measurable for (at least a subset of) the training set, but not for unseen objects and you would like to have a feature space or distance function in which pairwise distances correspond to this notion of similarity. The similarity measure should be expressible as a distance $\in \mathbb{R}^{+}$ (positive real number or zero). If the precise similarity is not known but a discrete metric can be established (e.g. objects being labeled the same or different), then discrete metric learning can be used instead of the proposed method. If we know the pairwise similarity for all examples, but we do not have a representation for the objects, data visualization techniques such as \ac{MDS} can be used to find a n-dimensional mapping for the objects in which distances between objects correspond to pairwise similarity.








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% BACKGROUND
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagebreak
\section{Background}

\subsection{Metric Learning}
\label{sec:metric_learning}

Many machine learning methods depend on pairwise comparisons between object representations. 

Euclidean distance does not always correspond with semantic distance or similarity.

The aim of metric learning methods is to learn a distance function helpful to the problem.

%local vs global, supervised vs unsupervised, linear vs non-linear

\subsubsection{Notation and problem formulation}

The problem of metric learning is to learn the parameters for a distance function from training examples.

A distance function, or metric, is a function mapping from a pair of coordinate vectors to a scalar, having certain properties.

The Mahalanobis metric is a linear parametric distance function and is most commonly used in metric learning.

% Linear metrics can be interpreted as a linear transformation on the input space.

% Although some methods learn 

structured prediction, predict complex outputs, sequences or trees, combinatorially many possible outputs, 0-1 loss not applicable, metric 

%need to parameterize the distance function, need some form of training data
%parametric distance metric: Mahalanobis distance, changing the matrix $M$ changes the distance metric, we can learn $M$ such that the distance satisfies our constraints
%Suppose we have a pair of vectors $\vec{x}, \vec{y} \in \mathbb{R}^D$
%\begin{equation}
%d_{M}(\vec{x},\vec{y}) = \sqrt{(\vec{x}-\vec{y})^{T} \mat{M} (\vec{x}-\vec{y})}
%\label{eq:mahalanobis_distance}
%\end{equation}
%metric: identity of indiscernibles: d(x,y) = 0 => x = y, symmetric: d(x,y) = d(y,x), non-negative: d(x,y) >= 0, sub-additive (triangle inequality): d(x,y) + d(y,z) >= d(x,z)
%pseudo: relaxes identity of indiscernibles to: d(x,x) = 0, meaning that there might be y not equal to x such that d(x,y) = 0
%Mahalanobis distance is pseudo-metric if $M$ is positive semi-definite and metric if $M$ is positive definite
%Linear metric, can be rewritten as linear transformation of the input space, comes in handy for nearest neighbor retrieval since we can still use Euclidean distance between transformed points
%rewrite $M = L^T L$, learn L, M is always PSD and the metric thus a pseudo metric, L is a linear transformation of the feature space such that Euclidean norm of transformed difference vector (or difference between transformed vectors) corresponds to $d_M$
%\begin{equation}
%d_{M}(\vec{x},\vec{y}) = ||\mat{L}(\vec{x}_{i}-\vec{y}_{i})||
%\label{eq:mahalanobis_transformation}
%\end{equation}
%$||\mat{L}(\vec{x}_{i}-\vec{y}_{i})|| = ||\mat{L}\vec{x}_{i}-\mat{L}\vec{y}_{i}||$


\subsubsection{Learning from equivalence constraints}

Suppose we have a problem with discrete labels for the examples, for classification or clustering we would like distances between examples to be small if their labels are the same and large otherwise.

\todo{Describe these methods (related work)}

%suppose we have a problem with input space $\mathcal{X} \subseteq \mathbb{R}^n$ and as output discrete class labels $\mathcal{Y} = {1, 2, ..., C}$. Then we want distance to be small iff labels are the same. kNN example, clustering example
%training set: subsets or pairs of objects which are similar. Many methods also use dissimilarity information: subsets or pairs of objects which are dissimiilar.
%try to make within-class variance small, difference between different classes large
%very applicable for classification tasks, but not so much for non-binary task such as those with a structured loss function, have to select some arbitrary cut-off for similarity or dissimilarity

\subsubsection{Learning from ranking constraints}

Suppose we have a problem without discrete labels but in which we try to find the most similar examples, we would like the distance to a similar example to be smaller than the distance to a less similar example.

\subsubsection{Learning from distance constraints}

Metric learning method generally don't do this, however MDS and visualization methods seem related in that they learn from pairwise distance matrix

\subsection{Nearest neighbor classification}

\subsection{Semantic segmentation}

%training set: ranking or triples of objects for which one is closer to the other
%try to satisfy as many of these constraints as possible


\subsection{SGD}
 \cite{rakhlin2012making} ASGD with suffix averaging is O(1/T) \cite{bottou2010large} ASGD or SGDQN is good for large scale problems \cite{bottou2008tradeoffs} SGD algorithms work surprisingly well in large scale learning \cite{xu2011towards} ASGD with specific parameters is optimal

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% METHOD
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagebreak
\section{Metric regression}
\label{metric_regression}

%What is the problem with other metric learning methods?
%input to metric learning methods are equivalence or ranking constraints, qualitative similarity function that establish relation between pairs or triples of points, metric learning does not take advantage of continuous distance measure, discrete metric learning, have to set arbitrary threshold, ranking learning, easier to learn from a restricted sample if you have more specific information, have to generate a lot of training data, mostly just interested in brining similar points together, don't have any specific target distance function, what if you do have samples from target distance, how can you leverage this?

In existing metric learning methods the input is a set of equivalence constraints or ranking constraints. Equivalence and ranking constraints are useful when we are interested in a qualitative similarity function, such as when points are assigned to a single class and we want the distance between same-class points to be small. But, in order to apply metric learning to problems where we are interested in a real-valued similarity function, we will have to generate equivalence or ranking constraints. Ranking constraints can be generated by sampling points and sorting them according to their similarity value, while equivalence constraints can be generated by setting a threshold on the similarity value. This approach introduces extra parameters and ignores the exact similarity values, thereby throwing away useful information.

%How does our method solve this problem? What is the idea behind our method?
%metric regression learns from real-valued distances, sampled from target distance, idea is to make Euclidean distance in feature space predictive of target distance, assumes that target metric is also a valid pseudo metric in some target space which is a linear transformation of feature space, however if not then we can still find the best linear transformation, but stay linear to keep methods efficient, just a single transformation of the features and can still use the same methods for finding neighbors/clustering

With metric regression we take a different approach. We do not ignore the exact values, but instead train on a set of distance constraints. Distance constraints are generated by sampling pairwise similarities and encoding them as real-valued distances. These distances are then used as target distance in a regression problem. The regression problem minimizes the difference between the target distances and the learned metric distance. Like other metric learning methods we parameterize our metric as a Mahalanobis metric:  \todo{I'll probably introduce Mahalanobis distance in section 2: "Metric learning background". Move this there.}
\begin{equation}
d_{\mat{M}}(\vec{x}_i, \vec{x}_j) = \sqrt{(\vec{x}_i-\vec{x}_j)^T \mat{M} (\vec{x}_i - \vec{x}_j)}.
\label{eq:mahalanobis}
\end{equation}
By substituting $\mat{M} = \mat{L}^T \mat{L}$ we can compute the Mahalanobis distance as Euclidean distance in a linearly transformed input space:
\begin{equation}
d_{\mat{L}}(\vec{x}_i, \vec{x}_j) =  ||\mat{L} (\vec{x}_i - \vec{x}_j)||_2.
\label{eq:mahalanobis_transformation}
\end{equation}
This allows us to apply the metric regression method as a linear transformation on the input space in a pre-processing step, and thus to use efficient methods for approximate nearest neighbor search or clustering that have been developed for Euclidean space. %Furthermore, these linear approximations have already been proven to be powerful in metric learning for discrete classes and rankings.

%How well does it work?
%toy example set that is easy to visualize, show some images of before and after transformation, also for ranking and LMNN, or otherwise show example from real data that happens to be a clear example
\todo{Show an example of effect on a toy set, a la the squares and circles in Schultz and Joachims paper.}

\subsection{Learning from pairwise distance constraints}

%What are the details of the method?
%define target distance as some function which we can sample, minimize empirical risk over this training data, global vs local sampling, training data, pairs and real-valued distance, input as pair differences, minimize using gradient descent, assuming that it is easy to sample this is large scale learning because number of pairwise distances is exponential in number of points, we use SGD, Frobenius norm regularization, many parameters: exponential in number of features, hence regularization,

A pairwise distance constraint is a tuple $(\vec{x}_i, \vec{x}_j, d_{ij})$ consisting of a pair of points in the input space $\vec{x}_i, \vec{x}_j \in \mathcal{X}$ and their target distance $d_{ij} \in \mathbb{R}_0^+$. \todo{How to get these constraints, sampling globally vs locally.}

Given a training set $\mathcal{D}$ of pairwise distance constraints we want to learn a linear transformation $\mat{L}$ that makes distances in the transformed input space predictive of target distance. We define the loss between predicted distance and target distance as squared error, thus our empirical risk corresponds to the mean squared error over the training set:
\begin{equation}
\text{MSE}_\mathcal{D}(\mat{L}) = \frac{1}{|\mathcal{D}|}\sum_{(i,j) \in \mathcal{D}} \left ( d_{ij} - ||\mat{L} (\vec{x}_i - \vec{x}_j)||_2 \right)^2.
\label{eq:mse}
\end{equation}
We fit the parameters of the transformation matrix $\mat{L}$ to the training set by minimizing the empirical risk. Thus our output transformation is given by $\hat{\mat{L}} = \argmin_{\mat{L}} \text{MSE}_\mathcal{D}(\mat{L})$. 

\subsubsection{Optimization by stochastic gradient descent}

In order to fit the transformation matrix to the training set we use \acf{SGD}. Gradient descent is an iterative first order optimization algorithm that finds a local minimum of an error function. On each iteration it changes the parameters by a small step in the direction opposite to the gradient of the error function with respect to the parameters. Since the negative gradient is the direction of steepest descent, each step leads to a reduction in error. When the error function is defined as a sum of individual errors on examples in a training set, \ac{SGD} is a stochastic version of gradient descent where at each step the gradient is computed on the error of a single example. This does not require a summation over all examples in the training set for each update, which is beneficial when the training set is large. \todo{Highlight benefits of SGD. Discuss small-scale vs large scale learning, relevant papers of Bottou and Xu. Probably move this to background and have a SGD subsection. Should also discuss ASGD and convergence speed.}

In order to apply \ac{SGD} to our learning problem we define an error function which has the same minimum as (\ref{eq:mse}) but is easier to differentiate and optimize because of its form:
\begin{equation}
E_{\mathcal{D}}(\mat{L}) =  \sum_{(i,j) \in \mathcal{D}}  \frac{1}{4} \left ( d_{ij}^2 - ||\mat{L}(\vec{x}_i - \vec{x}_j)||_2^2 \right )^2.
\label{eq:opt_target}
\end{equation}
Here $\vec{x}_i - \vec{x}_j$ is the pairwise difference between two vectors in the input space $\mathcal{X}$ and $d_{ij}^2$ is the squared target distance for this pair of input vectors. $||\mat{L}(\vec{x}_i - \vec{x}_j)||_2^2$ is the squared Euclidean distance in the transformed space which is efficient to compute as the inner product of the transformed difference vector. The constant vector $\frac{1}{4}$ is there only for convenience in differentiation. 

Interpreting the optimization objective as $E_{\mathcal{D}} = \sum_{(i,j)\in\mathcal{D}} E_{ij}$ the corresponding gradient of the error $E_{ij}$ on a single pairwise distance constraint $(i,j) \in \mathcal{D}$ is given by:
\begin{equation}
\nabla E_{ij}(\mat{L})  = - \left ( d_{ij}^2 - ||\mat{L}(\vec{x}_i - \vec{x}_j)||_2^2 \right) \mat{L} \mat{C}_{ij},
\label{eq:opt_grad}
\end{equation}
where $\mat{C}_{ij} = (x_i - x_j)(x_i - x_j)^T$ is the outer product of the pairwise difference.

We initialize our transformation as the identity transformation $\mat{L} = \mat{I}$ and then update it at each iteration as:
\begin{equation}
\mat{L} \leftarrow \mat{L} - \eta_{t} \nabla E_{ij}(\mat{L}).
\label{eq:update}
\end{equation}
Here $t$ is the iteration number, $\nabla E_{ij}(\mat{L})$ is the gradient computed over the distance constraint for points $(i,j) \in \mathcal{D}$ and $\eta_{t}$ is the learning rate: a gain parameter which controls the step size. The learning rate should decrease and the schedule with which it decreases is important for the convergence speed of the optimization \cite{xu2011towards}. We use the parameterization proposed by \cite{xu2011towards} and calculate $\eta_{t}$ at each timestep $t$ as follows:
\begin{equation}
\eta_{t} = \frac{\eta_0}{\left(1+ a \eta_0 t \right)^c}
\label{eq:eta_update}
\end{equation}
For SGD without regularization we set $a = 1$ and we leave the parameter $c$ as a hyper parameter that needs to be tuned. Following \cite{bottou2008tradeoffs} we set $\eta_0$ by optimizing it on the first thousand points in the dataset and selecting the value that reduces loss the most.

\subsubsection{Relative error}

The squared error measure in (\ref{eq:mse}) assigns greater importance to larger pairwise differences. This might not be optimal for nearest neighbor applications since the points with smallest pairwise difference are most important for classification. \todo{Mention stress functions which have parameter for this.} In order to put equal weight on different length difference vectors, we define a squared relative error as loss function:
\begin{equation}
\text{MSRE}(\mat{L}) = \frac{1}{|\mathcal{D}|}\sum_{(i,j) \in \mathcal{D}} \left ( \frac{||\mat{L} (\vec{x}_i - \vec{x}_j)||}{d_{ij}} - 1 \right)^2.
\label{eq:msre}
\end{equation}
Analogous to (\ref{eq:opt_target}) we create an optimization objective $R_{\mathcal{D}}(\mat{L}) = \sum_{(i,j) \in \mathcal{D}} R_{ij}$ which leads to the following gradient $\nabla R_{ij}(\mat{L})$ for the squared relative error on the pairwise distance constraint $(i,j) \in \mathcal{D}$:\todo{Need to handle asymptote at $d_{ij} \rightarrow 0$.}
\begin{equation}
\nabla R_{ij}(\mat{L})  =  \left ( \frac{||\mat{L}(\vec{x}_i - \vec{x}_j)||_2^2}{d_{ij}^2} - 1 \right) \frac{1}{d_{ij}^2} \mat{L} \mat{C}_{ij},
\label{eq:relative_grad}
\end{equation}
In contrast to (\ref{eq:opt_grad}) this scales the residual error and gradient by the magnitude of the squared target distance $d_{ij}^2$. Using relative error leads to larger steps for updates computed on small distances, making nearest neighbors more important in the optimization.


\subsubsection{Regularization}

%learn on training set, distances are sampled, might contain noise, might not be entirely in line with real distribution, might contain sampling bias especially for local sampling, hence possibility for overfitting, many parameters ($D^2$ for a $D$-dimensional input space) 
The target distances in the training set need not be a perfect representation of the actual distribution of distances. The sampling process might be noisy and the samples might not be independent or identically distributed. Furthermore, the number of parameters that we fit is large: $D^2$ for a $D$-dimensional input space. This flexibility in combination with imperfect training data might lead to \emph{overfitting}. The optimization continues to decrease error on the training data, but error on unseen data (e.g. a validation set) stops decreasing and starts increasing as the learned transformation becomes more specific to the irregularities of the training data and less generally applicable.

One way to prevent overfitting is by early stopping, for example by monitoring the error on a separate validation set and stopping once this error no longer decreases. Another way that we describe here is by incorporating a regularization penalty into the objective function that we optimize. For this we define a cost function that penalizes large transformations:
\begin{equation}
C_{\mathcal{D}}(\mat{L}) =  E_{\mathcal{D}}(\mat{L}) + \lambda ||\mat{L}||_\textsc{f}^2.
\label{eq:structured_risk}
\end{equation}
Here $C_{\mathcal{D}}(\mat{L})$ is the cost function that we will minimize, $E_{\mathcal{D}}(\mat{L})$ is the error on the data, $ ||\mat{L}||_\textsc{f}^2$ signifying the Frobenius norm of $\mat{L}$ is the regularization penalty and $\lambda \in \mathbb{R}_0^+$ is a parameter that determines the strength of the regularization. The gradient for the update then becomes:
\begin{equation}
\nabla C_{ij}(\mat{L}) =  E_{ij}(\mat{L}) + \lambda 2\mat{L}.
\label{eq:structured_grad}
\end{equation}


\subsubsection{Mini-batch optimization}

\todo{This should also be in the SGD background section, making it just a parameter which we'll discuss further in the tuning section.}

\begin{equation}
\mat{L} \leftarrow \mat{L} - \eta_{t} \frac{1}{|\mathcal{B}_t|} \sum_{(i,j) \in \mathcal{B}_t} \nabla E_{ij}(\mat{L})
\label{eq:update}
\end{equation}

\bibliographystyle{plain}
\bibliography{Thesis}




%suppose a similarity measure which gives a real valued measure of similarity between two objects, inverted (dissimilarity measure) gives us a distance function or metric that we want to learn,
%uses training data in a way that makes the method applicable to problems characterized by real-valued distance functions, such as a structured loss function
%Applicable to problems characterized by real-valued distance functions, such as a structured loss function.
%We want distance to be inverse proportional to similarity. 
%Large margin approaches lead to dissimilar examples having sufficiently larger distance than similar examples, but does not enforce the more dissimilar examples to be further away than slightly dissimilar examples proportionally to their difference in dissimilarity. 
%Ranking constraint approaches do enforce this ranking, but doesn't enforce the proportionality.
%Metric regression directly approximates given real-valued dissimilarity measure and therefore satisfies all these constraints.
%we would like examples with a larger loss to have greater distance.

%Metric regression is posed as a simple convex optimization problem which can be solved without special purpose solvers.
%Solution can be written as an unconstrained convex optimization problem.

%training set: pairs of objects with given distance (not similarity since we learn a distance function)
%$\mathcal{X}_{\textsc{train}} \subseteq \mathcal{X}$
% training tuples $\mathcal{T} = \{(\vec{x}_i, \vec{x}_j, d_{ij})\}$ with input vectors $\vec{x}_i, \vec{x}_j \in \mathcal{X}_{\textsc{train}}$ and their pairwise distances $d_{ij} \in \mathbb{R}_{0}^{+}$.
%Suppose we have a problem in which we are interested in the similarity between input examples. These input examples are represented through a set of real-valued attributes, thus each example corresponds to a vector  $\vec{x} \in \mathbb{R}^n$ in the representation space. We now want to measure the similarity between two input vectors $\vec{x}_i$ and $\vec{x}_j$. Without any prior knowledge we could measure the Euclidean distance between 
% compare $d(\vec{x}_i, \vec{x}_j) = ||\vec{x}_i - \vec{x}_j||_2$
%learn a distance metric specific to the problem instead of relying on 'default' metrics (e.g. Euclidean), same data set and features but difffernt task
%learn M by minimizing SSE over this training set
%\begin{equation}
%M = \argmin_{M} \sum_{(\vec{x}_i, \vec{x}_j, d_{ij}) \in \mathcal{T}} \left(d_{ij} -  \sqrt{(\vec{x}_{i}-\vec{x}_{j})^{T} M (\vec{x}_{i}-\vec{x}_{j})}\right)^2
%\label{eq:min_sse_M}
%\end{equation}
%rewrite $M = L^T L$, learn L, M is always PSD and the metric thus a pseudo metric, L is a linear transformation of the feature space such that Euclidean norm of transformed difference vector (or difference between transformed vectors) corresponds to $d_M$
%\begin{equation}
%L = \argmin_{L} \sum_{(\vec{x}_i, \vec{x}_j, d_{ij}) \in \mathcal{T}} \left( d_{ij} -  ||L(\vec{x}_{i}-\vec{x}_{j})|| \right)^2
%\label{eq:min_sse_L}
%\end{equation}
%\begin{equation}
%L = \argmin_{L} \sum_{(\vec{x}_i, \vec{x}_j, d_{ij}) \in \mathcal{T}} \left( d_{ij}^2 - L\delta_{ij} \cdot L\delta_{ij} \right)^2
%\label{eq:min_sse_L}
%\end{equation}











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% EXPERIMENTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagebreak
\section{Application to semantic segmentation}

%metric regression, continuous distance functions, unknown or unavailable during execution, transforms feature space, feature distances are predictive of training distance
The metric regression method described in section \ref{metric_regression} is applicable to problems characterized by a continuous distance function. Given samples of pairwise distances, metric regression learns a transformation of the feature space which makes Euclidean distances between feature vectors predictive of the sampled distance function. This is helpful when the exact formulation of the target distance function is unknown or unavailable during execution. For example, when samples of the target distance consists of pairwise similarities judged by a human expert who can not give you an exact formula for his judging process. Or another example, when the target distance is defined on the attributes that should be predicted. In this case we would be able to calculate exact distances during training, but not during execution.

%classification problems with continuous loss function, nearest neighbor methods, make distance predictive of loss for predicting neighbor label
A common class of such problems are structured prediction problems with continuous loss functions. Structured prediction problems are classification problems where the goal is not to predict some single output label for each example, but the goal is to predict some element of a structured output space $\vec{y} \in \mathcal{Y}$ where $\vec{y}$ is some complex element, for example a sequence or a tree of labels. Because of the complexity of the output elements, the 0-1 loss measure $L(\hat{y}, y) = \left[\hat{y} \ne y\right]$ that is commonly used in classification is no longer applicable since it treats a sequence which differs in only one element the same as a sequence which differs in all elements. Hence, such problems define a continuous loss function $L: \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}$, for example Hamming distance for sequences of labels.

Now suppose we would like to apply \acf{kNN} classification to such a structured problem. In \ac{kNN} classification we predict an output by looking at the output corresponding to the $k$ closest training examples. Suppose we set $k = 1$ and we simply predict the output we have observed for the closest training example. This approach performs best when the closest example is the example with the lowest loss relative to the ground truth label. However, we do not have this loss available during execution to rank neighbors, thus `closest' is defined here as smallest distance in feature space. Hence, if we train our metric to be predictive of the pairwise loss, we might increase our performance.

%semantic segmentation, per-pixel classification, apply \ac{kNN} to image patches, patch has feature descriptor and label patch, labels correspond to object classes, loss is distance in label space, align feature space to label space, improved segmentation accuracy
In this section we show how to apply metric regression to semantic segmentation, a structured prediction problem where the objective is to classify each pixel in an image. Here we solve this problem by dividing the image into overlapping patches and classifying each patch using \ac{kNN} classification. For each patch we compute a feature vector and we use metric regression to learn a transformation of these feature vectors which aligns the distances in this feature space with loss in label space. We show that this simple linear transformation improves alignment between Euclidean distance between examples and pairwise loss and that it improves overall segmentation accuracy.



\subsection{Semantic segmentation background}

%semantic segmentation, challenging problem, combines object recognition and image segmentation, applications in image understanding, image retrieval
Semantic segmentation is a challenging problem that combines object recognition and image segmentation, with applications in image understanding and image retrieval. The objective is to segment input images into contiguous regions by assigning labels to each pixel from a predefined set of class labels, in effect performing a per-pixel classification of the image. Figure \ref{fig:segmentation_examples} shows some example input and output for the semantic segmentation task, and the semantic segmentation problem is described below in more detail.

\begin{figure}[tbh]
\begin{center}
\missingfigure{Top row: image from MSRC, VOC, CamVid. Bottom row: corresponding segmentations.}
\caption{Some examples of the semantic segmentation task. Top row: input images from the MSRC, Pascal VOC, and CamVid benchmarks. Bottom row: the manual segmentations corresponding to the input images. Each color denotes a different object class. Note the difference in annotation style between the benchmarks.}
\label{fig:segmentation_examples}
\end{center}
\end{figure}

\begin{quotation}
\subsubsection*{Semantic segmentation problem.}

\paragraph{Input:} An image $\mat{I}$ where each pixel $\mat{I}_{i,j}$ is usually given as a triple $(r,g,b)$ denoting the color of a single pixel. And a label set $\mathcal{L} = \{1, 2, \ldots, n\}$ where each label corresponds to one of $n$ pre-determined object classes.

\paragraph{Output:} A segmentation image $\mat{S}$ of which each element  $\mat{S}_{i,j} \in \mathcal{L}$ assigns a label to pixel $\mat{I}_{i,j}$ of the image.

\paragraph{Learning problem:} given a set of training images $\mathcal{I}_\textsc{train} \subset \mathcal{I}$ with the corresponding ground truth segmentations $\mathcal{S}_\textsc{train} \subset \mathcal{S}$ and a loss function $L: \mathcal{S} \times \mathcal{S} \rightarrow \mathbb{R}$, learn a function $h(\mat{I}) = \mat{S}$ that minimizes this loss over unseen images. %\todo{...minimizes this loss over true distribution of images.}
\end{quotation}

% common approaches, structured learning CRF, tree ensembles with context features, segmentation with object detectors, clustering
The main challenges in semantic segmentation are a combination of the challenges found in object recognition, image segmentation, and structured prediction. Like in object recognition there is high variance in appearance, orientation and shape of objects belonging to object classes of interest. However, since single pixels have to be labeled, there is less information available for making a labeling decision. Semantic segmentation methods solve this either by pre-segmenting the image into superpixels or larger segment hypotheses such that evidence, e.g. object recognition response, can be accumulated over a larger area, or by using features which aggregate information about a larger area around the pixel, its context. Like in structured prediction problems, maintaining consistency between predicted labels is also an important challenge. Because of this, many semantic segmentation methods use a \ac{HCRF} approach which allows the penalization of inconsistency on multiple levels unless there is strong evidence for non-continuity in the labeling.\todo{Give some global background describing challenges of segmentation (much variance in appearance and shape, label consistency, finding correct scales for segmentation) and the main approaches to solving them (HCRF, superpixels, SVM, which differ by benchmark).}\todo{Segmentation box? Show different segmentation tasks and talk about basic methods like superpixels and clustering methods?}

\subsection{$k$-Nearest neighbor segmentation}

%segmentation with kNN, kNN is simple, requires little parameter tuning, and is applicable to multi-class problems, 
Our segmentation method is based on \acf{kNN} classification. \ac{kNN} is a non-parametric classification method, which means that its decision function is not characterized by a parameterized model. Instead, the class for an input point is decided to be the majority class amongst the most similar points in the training set, the nearest neighbors. This tactic is simple, requires little parameter tuning and is inherently applicable to multi-class problems. The down-side is that it needs to store the whole training set and that it has to find nearest neighbors among that set during execution. This becomes more troublesome with larger training sets. \todo{What is the exact big-O runtime complexity for randomized kd trees?}\todo{Box: the k-nearest neighbor classification?}

%we use a simple generalization to structured prediction, this is patch based, find nearest patches and interpolate each label of the patch independently
We generalize \ac{kNN} to the case of structured prediction by applying it to structured label patches, following the idea in \cite{kontschieder2011structured} of generalizing random forests to use structured label patches. We define a structured output space $\mathcal{Y} = \mathcal{L}^{d \times d}$ consisting of label patches $\vec{y}$ whose elements $y_{i,j} \in \mathcal{L}$ each consist of one of the pre-determined class labels. When predicting the label patch $\hat{\vec{y}}$ for a given input vector we look up the label patches of the $k$ nearest neighbor vectors $\mat{Y}_\textsc{nn} = {\vec{y}_1, \ldots, \vec{y}_k}$ and interpolate each label position independently based on the labels at the same position of the retrieved patches:
\begin{equation}
\hat{y}_{i,j} = \argmax_{\ell \in \mathcal{L}} \sum_{\vec{y} \in \mat{Y}_\textsc{nn}} \left[ y_{i,j} = \ell \right ].
\label{eq:nn_decision_rule}
\end{equation}

\begin{figure}[tbh]
\begin{center}
\missingfigure{Figure explaining searching for nearest neighbor vectors in feature space and patch creation from neighbor patches according to equation \ref{eq:nn_decision_rule}}
\caption{Schematic overview of the \ac{kNN} algorithm with structured label patches. }
\label{fig:knn_segmentation}
\end{center}
\end{figure}

%for each of the training images extract $d \times d$ image patches from $I$ on regular grid $(\vec{u}, \vec{v})$, compute array of image features $X = \left [ \vec{x}_1, \vec{x}_2, \ldots, \vec{x}_p \right ]$ with $\vec{x}_i$ computed from the image patch centered on image coordinates $(u_i, v_i)$, use SIFT and CSIFT features, also extract label patches from the corresponding image labeling $L$ such that label patch  $\vec{y}_i$ corresponds to the $d' \times d'$ label patch around $(u_i, v_i)$, store these coordinates and create index for the features such that we can efficiently find the closest points
From the training images we extract feature vectors $\mat{X}_\textsc{train} = \left [ \vec{x}_1, \vec{x}_2, \ldots, \vec{x}_p \right ]$ on a regular hexagonal grid $(\vec{u}, \vec{v})$, with $\vec{x}_i$ computed from the image area centered on image coordinates $(u_i, v_i)$. Each of these feature vectors $\vec{x}_i$ is paired to a label patch $\vec{y}_i$ of size $d \times d$ taken from the training annotation for the image and centered at $(u_i, v_i)$. For convenience we define these patches to have the origin $y_{0,0}$ in the center of the patch, such that the subscripts $i$ and $j$ run from $-\tfrac{d}{2}$ to $\tfrac{d}{2}$. We store all the training vectors and label patches and we create an index for the feature vectors such that we can efficiently find the closest points from this set given an input vector.

%for each test image, again compute features for each patch, for each feature find the closest patch in the training set, extract label patch from the same area, merge overlapping label patches to create the output labeling
For each test image $\mat{I}$ we again extract feature vectors $\mat{X}_\mat{I}$ on the grid $(\vec{u}, \vec{v})$. For each of these feature vectors we search for the nearest neighbors amongst the training vectors and their associated label patches. From these we then generate the hypothesis patches as per (\ref{eq:nn_decision_rule}) giving us a label patch centered at each grid point $(u_i, v_i)$. These hypothesis patches are partially overlapping. Thus for each pixel in the image we select the most voted class among the positions in patches that overlap that pixel, similar to (\ref{eq:nn_decision_rule}).

\begin{algorithm}
\caption{The structured k-nearest neighbor algorithm for label patches.}
\label{alg:structured_kNN}
\begin{algorithmic}[1]
\REQUIRE  $\mat{X}_\textsc{train}$, $\mat{Y}_\textsc{train}$, $\mat{X}_\mat{I}$ 
\ENSURE $S = h(\mat{I})$
\STATE TODO: give clear pseudocode
\end{algorithmic}
\end{algorithm}

\subsection{Aligning distances in feature space to loss}

%effectiveness of \ac{kNN} method highly dependent on representation function, meaning close in feature space has to align with semantically close, if appearance and semantics are not aligned this will lead to wrong prediction based on nearest neighbors
It is clear that the effectiveness of the structured \ac{kNN} method described above is highly dependent on both the representation function and the distance function. The representation function defines the feature space, the space in which distances between the input vector and training vectors are measured, and the distance function defines the way in which these distances are measured. In order to be effective, our method needs semantically similar points to lay close together in feature space, with `semantically similar' meaning that the corresponding label patches have a large overlap.

%use metric alignment to learn feature transform, label patches, loss defines distance in label space, we want small distance to mean small loss
The naive approach is to use Euclidean distance as a default and then to search for features which make similar patches lie close together. However, it is very difficult to engineer such an image feature. Instead, we will use existing features which have been applied in many image classification tasks and use the metric regression method described in section \ref{metric_regression} to learn a distance metric which makes distance between feature vectors predictive of loss between the corresponding labels. We define this loss as a function with domain $[0,1]$ which gives us an inverse of the overlap between label patches:
\begin{equation}
L(\vec{y}, \vec{y}\prime) = 1 - \frac{\sum_{i,j} \left [ y_{i,j} = y\prime_{i,j}\right]}{\sum_{i,j} \left [y_{i,j}, y\prime_{i,j} \in \mathcal{L}\right ]}.
\label{eq:patch_loss}
\end{equation}
By training the distance metric on this pairwise loss, we try to align the feature space to the label space in such a way that when we will look for nearest neighbors in feature space we will return neighbors that have label patches that are close to the correct label patch for the input feature vector.


%%%%
%%%%
%%%%


\subsection{Experimental setup}

%We evaluate our method by applying our segmentation pipeline to benchmark problems with Euclidean distance and with distance metric learned by our metric alignment method.
%learning to rank, LMNN, RF (Kontschieder et al.)
To evaluate our method we perform an experiment using two semantic segmentation benchmarks. In this experiment we apply the \ac{kNN} segmentation method described above using a distance function learned by applying metric alignment on the training data. We compare this against the result of applying the \ac{kNN} method using standard Euclidean distance.

%We compare our method against two baseline large-margin metric learning methods: LMNN \cite{weinberger2009distance} which learns from discrete examples and \cite{schultz2003learning}
Besides the Euclidean baseline we also compare against two alternative metric learning methods, both of which use a large margin approach to metric learning. First against \ac{LMNN} \cite{weinberger2009distance}, described in section \ref{discrete_metrics}, and secondly against \cite{schultz2003learning}, described in section \ref{ranking_metrics}. We use \ac{LMNN} as a baseline because it is a well tested method designed specifically to improve \ac{kNN} classification results. The second method was chosen because it is similar in spirit to our own approach in that it learns from relative comparisons, but it is different in practice since it uses qualitative training data while metric alignment uses quantitative data.

%We evaluate both the accuracy of the learned metrics on pairwise differences and the effect of the metric learning on the segmentation task using our \ac{kNN} segmentation method.
To compare the different methods in this experiment we look at how well the distance function between image patches approximates the label distance as measured by \ac{MSE} as well as the overall increase in segmentation accuracy as measured by the benchmark evaluation measures.



\subsubsection{Datasets}

%We evaluate our method on the MSRC and VOC segmentation datasets; two benchmarks in semantic segmentation.
To evaluate our methods we use two datasets of images which have been manually segmented into  \footnote{\url{http://research.microsoft.com/en-us/projects/objectclassrecognition/}}

The MSRC set consists of a small number of images, loosely segmented into 21 classes. covering most of each image with little variation in appearance and scene.

Camvid dataset

The VOC set consists of a large number of images with strict segmentation into 20 foreground classes and an aggregate background class with much variation in appearance. %Object detection challenge, hence background class, but this is challenging for segmentation methods because you have to decide what not to segment.

Difference in annotation styles, MSRC is loosely segmented while the Camvid and VOC are close to pixel-perfect. VOC generally has one or a few foreground objects segmented while the rest is grouped as background, while MSRC and CamVid are fully segmented with background classes being labeled, e.g. `grass', `street'. See figure \ref{segmentation_examples}.

VOC is more challenging than MSRC and CamVid on both effectiveness and sheer size.


\subsubsection{Implementation}

We use SIFT and C-SIFT image descriptors as feature vectors extracted using the ColorDescriptor software. \cite{sande2011empowering} We then index these training vectors using the multiple kd-tree method from the FLANN library \cite{muja2009fast}, which allows us to efficiently calculate nearest neighbors amongst these points given an input vector.

\todo{Box: image descriptors?}

\subsubsection{Training and parameter selection}

We divide the dataset into non-overlapping training and testing images. 


extract training pairs from training images, note: can't use all patch pairs because this are intractably many, we use a set of randomly selected points and their k-nearest neighbors

train on feature vectors and pairwise loss, compute gradient over the neighborhood set at each step

\todo{Explain tuning.}






\subsubsection{Evaluation}



We measure the \ac{MSE} between the label distance and the learned nearest-neighbor on a sample of testing pairs

%nearest neighbor MSE histogram?

we evaluate global pixel accuracy and average class pixel accuracy as well as the intersection over union measure used in the pascal VOC dataset

we show per patch results for different numbers of neighbors 

plot of label distance vs learned metric distance, both normalized\

plot of segmentation accuracy as a function of neighbors?

plot of distance to closest ground-truth patch?





%%%%
%%%%	
%%%%








\subsection{Results}

%Training curve with MSE on training pairs

%MSE on patch overlap for testing pairs

%effect on segmentation score

%effect of features and dimension reduction on learnability

%effect of parameters for minimization, not all training schemes seem to work










































%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% CONCLUSIONS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagebreak
\section{Conclusions}

%list the specific cases/requirements in which this is useful (see intro)















\bibliographystyle{plain}
\bibliography{Thesis}


%\appendix
%\section{Glossary}

%\begin{description}
%\item[Similarity measure] A measurement of the similarity between two objects. 
%\item[Distance function] A function which maps pairs of points in some space to a positive real number or zero.
%\item[Metric] See \emph{distance function}.
%\item[Metric transformation] A linear transformation of a feature space for which it holds that Euclidean distances in the transformed space correspond to a specified distance function on the original space. 
%\end{description}

\end{document}  