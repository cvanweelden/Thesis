\chapter{Conclusions}

In this thesis we investigated the idea of learning a metric from absolute distance constraints instead of the binary similarity/dissimilarity or relative constraints that are in common use. Some problems, especially structured prediction problems, do not allow for the simplistic approach of dividing data points based on class membership. The complex nature of the structured output space calls for a continuous loss function instead of the 0/1-loss used in simple classification problems. In order to adapt metric learning to such problems we introduced the notion of absolute distance constraints which take the continuous loss function as target metric.

We show that a metric can be learned from the absolute distance constraints by casting this problem as a regression problem on the squared prediction error in regard to the target metric. This can be solved by decomposition of the Mahalanobis matrix such that the metric can be optimized through a stochastic gradient descent algorithm.  Our first experiment shows that our method learns a metric that minimizes this the mean of the squared prediction error.

However, the squared measure that we minimize seems vulnerable to outliers. Although our results show that alignment to the target metric is correlated to performance on the prediction task, it also shows that minimizing the mean squared error might not optimize the alignment. This in turn hurts the prediction performance.

Although the learning tasks presented in this thesis proved too challenging for the proposed algorithm, it can be applied to metric learning problems characterized by a continuous loss function, such as structured prediction problems, which can not be handled with metric learning methods relying on binary similarity/dissimilarity constraints or large margin relative constraints. The learning tasks that we introduce form a challenging testing ground for further development in this direction, with the problem lying in both the learning of a metric which has a positive effect on the prediction accuracy, as in the sheer size of the constraint set that is generated which makes efficient optimization difficult.