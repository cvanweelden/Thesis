


%3.5(bad) student network example
%3.7 proposed method should be used when you need a distance function measuring similarity or a feature space in which Euclidean distances correspond to similarity between objects for which it is difficult to decide whether they belong to the same discrete class and the similarity you want to measure is known for at least a subset of the training set but not for new examples but training and test objects can be represented by the same set of features.

%4.1 metric alignment, continuous distance functions, unknown or unavailable during execution, transforms feature space, feature distances are predictive of training distance
%3.4-2 metric alignment, learn a metric that satisfies real similarity score



%dimension reduction and visualization methods, (multi-dimensional scaling, isomap, LLE, SNE), mapping/embedding retaining pairwise distances, finding coordinates in some space which satisfy a similarity score



\section{Introduction.}

%2.1 objects represented as vectors of feature values, feature space
%1.1-1 machine learning features, feature space,
In machine learning tasks, objects of interest are represented using a set of features. Features correspond to attributes of the object that can be encoded in some way. For example the features representing images may simply be the pixel values of an image, but may also be more complex features computed from the basic pixel values. Each object is thus represented by a feature vector containing the feature values. We will refer to the space of all possible feature vectors as the \emph{feature space} and in the case of $d$-dimensional real-valued vectors take this space to be $\mathbb{R}^d$. \todo{Too abstract, might loose some readers in the first paragraph. Too fast into the mathematical notation. Maybe start with application. Show an image of the example (maybe replace cars with the animals set). Try to also show ground truth metric in an image.}

%2.2-1 measure similarity by assuming small difference in feature vector means similar objects,
%1.1-2 measuring distance, finding similar samples, Euclidean distance
%computing distance between data samples fundamental problem
%3.1-1 similarity between objects, distance function,
Now suppose we would like to measure similarities between objects. For example, we have some images of a Ferrari car and we want to find images of similar cars. Assuming that similar objects result in similar feature values, we can express similarity between objects as a distance function in feature space. For our $\mathbb{R}^d$ feature space we can therefore measure similarity by Euclidean distance between the points in the feature space given by the feature vectors. Thus, starting from the feature representations of our input images we would expect to find images of similar cars nearer in feature space than images of dissimilar cars.

%3.2 representation of objects, feature space, distance in this space different from similarity, explain problem with clustering: picking different features or scaling will lead to different clusters which do not all correspond to our idea of similarity, common to use Euclidean or Hamming distance depending on the feature vector
%2.3-1 feature difference not always good measure for similarity,
However, Euclidean distance is not necessarily the best measure for similarity. In most cases features are predetermined since they measure basic attributes of the object, but the type of similarity we want to measure is specific to the task. With Euclidean distance each feature has an equal impact on the distance, even though some features are much more relevant to the type of similarity we want to measure than others. For example in our car images example, we would want to focus on similarities in color if we are simply looking for other red cars, but we would want to look at more complex similarities in shape if we are looking for other racing cars. 

%1.2 use of metric in machine learning methods, kNN, k-means, SVM, effectiveness depends on metric
%2.2-2 used by kNN and k-means
%3.1-2 applications (clustering, non-parametric methods based on distance or local density [kernel density estimation, k-nearest neighbor classification], visual identification)
Several machine learning methods use a distance function at the core of their algorithm. For example, the \ac{kNN} algorithm classifies objects by searching the training data for feature vectors that are closest to the input vector. The objects corresponding to these nearest neighbors then vote on the class to assign to the input object. It is clear that for this to be effective, distances in feature space should be small for objects of the same class and large for objects of differing class. Similarly, the $k$-means clustering algorithm clusters samples based on their distances in feature space. This will cluster similar objects together if they are close together in feature space. These methods require us to carefully choose a feature representation and distance function in order to be effective.

%1.3-1 metric learning, parametric distance function, learn parameters,
%2.3-2 metric learning: learn distance function that is more useful
%3.3-1 metric learning: learn a distance function between object representations that corresponds to actual similarity, metric, parametric metrics, metric transform, Mahalanobis distance,
\todo{Could sell this as: Since we have some background knowledge about what we want the distance function to be like, we can use this knowledge to automatically learn ...}
Instead of hand-crafting a distance function, we can automatically learn a suitable distance function in a given feature space. This approach is called \emph{metric learning}. Metric learning methods define a parametric distance function and then learn the best parameters for this function. Learning the parameters requires a set of training samples in the given feature space and a set of constraints on the distances between training samples. Parameters of the distance function are learned that best fit these distance constraints. The constraints thus represent the similarity measure specific to our task. For example, if we are still looking for images of similar racing cars, we would provide samples of racing cars and then constrain the distances between these to be small, while constraining the distances to other types of cars to be large.

%1.3-2 constraints based on class of samples
%2.4-1 train metric on class differences,
%3.3-2 using discrete similarity for training or a similarity ranking,
Metric learning has been researched most in the context of multi-class classification problems and ranking problems. Because of this background, the constraints generally take the form of similarity/dissimilarity constraints or relative distance constraints. Similarity and dissimilarity constraints are generated from class labels: objects of the same class are constrained to have a small pairwise distance while objects of different classes are constrained to have a larger distance. In contrast, relative distance constraints are used for tasks where it might be difficult to make absolute similarity judgments, but where we can identify one object to be more similar to a given object than some other object. Relative distance constraints constrain one pairwise distance to be smaller than another pairwise distance.

%3.4-1 what if we have exact similarity scores represented as real numbers, but not discrete classes, e.g. structured problem
%research questions: how can metric learning be applied to real-valued loss, how does this compare to existing problems
However, there is a more direct approach to the metric learning problem: instead of generating similarity/dissimilarity or relative distance constraints, a metric can be trained on absolute distance constraints that directly specify what the distance between a given pair of points in feature space should be. \todo{Put more emphasis on target metric. Next to having labels we also have an exact idea of what the metric should look like. We differentiate from other methods in that we can use a target metric and align to that, while other methods can not do this.} Absolute distance constraints make more fine-grained similarity judgments than the binary similarity/dissimilarity constraints while supplying stronger supervision than relative distance constraints. Despite these advantages, absolute distance constraints have received little attention in the metric learning literature. Therefore, this thesis investigates the use of absolute distance constraints for metric learning. In doing so it answers the following research questions:
\begin{itemize}
\item How can we apply metric learning methods to problems with absolute distance constraints?
\item How does metric learning with absolute distance constraints compare to methods using similarity/dissimilarity or relative distance constraints?
\end{itemize}

\begin{figure}[t]
\begin{center}
%\missingfigure{\tiny{Two 2D plots side-by-side. Left plot showing 4 points connected to a central point. Connecting lines are labeled by Euclidean distance, points are labeled by ground-truth distance. Top and bottom point should have large distance (e.g. 3,4) while middle points should have small distance (e.g. 1), but now all have about 2 distance. Right plot shows points in transformed space, the center points are now closer to 1 distance while the other two points are now closer to 3, 4 distance. Also show metrics as ellipses (covariance matrices) making clear that our method learns these parameters.}}
\includegraphics[width=\textwidth]{Figures/approach_overview_sketch}
\caption{The effects of metric alignment on a toy problem. The left plot shows the original space, while the right plot shows the original space transformed using the learned distance function. Points are labeled with target distance, while connections are labeled by Euclidean distance in the original and transformed space.}
\label{fig:approach_overview}
\end{center}
\end{figure}
\todo{Make a plot for Figure \ref{fig:approach_overview}.}

%4.2 classification problems with continuous loss function
%4.3 kNN for structured prediction, performs best when distance function is small if loss is small for k=1, have to predict loss from feature space
%1.4 real-valued loss function instead of nominal values for classes, metric proportional to loss, predict loss from feature space by learning metric on real-valued constraints
%2.4-2 we train on real-valued ground-truth metric, such as real-valued loss function in structured prediction problems
There is a specific class of machine learning problems that requires absolute distance constraints, namely \emph{structured prediction} problems. Structured prediction problems are classification problems in which the goal is not to predict a single label, but to predict a complex output structure. For example, predicting a label sequence. Unlike a single class label, which is always either correct or incorrect, structured outputs can be partially correct. Therefore, structured prediction problems define a real-valued loss function which measures the divergence between two output structures. When we apply a prediction algorithm our goal is to predict an output that has the lowest loss relative to a given ground-truth output structure. Modifying the \ac{kNN} classification algorithm to work as structured prediction algorithm is straightforward: instead of a simple voting scheme, the outputs corresponding to the $k$ nearest neighbors in feature space are combined in some task-specific way. Since our goal is to predict output structures that have the lowest loss relative to the ground truth, this method is most effective when small distances in feature space correspond to low loss. By generating absolute distance constraints which constrain distances between points in feature space to be proportional to the loss between the corresponding outputs, we can learn a distance function that ensures nearest neighbors in feature space correspond to low-loss solutions. We refer to this method as \emph{metric alignment} since satisfying these constraints aligns the feature space to the structured-output space in which the loss function is measured.

Figure \ref{fig:approach_overview} shows an example of the effect of learning a distance function using absolute distance constraints on a small toy problem. The plot on the left shows a set of training points in the original feature space. Each point is labeled with a target distance to the unlabeled point in the center. The connecting lines between the points and the center point are labeled with the Euclidean distance in the feature space. As can be seen, the target distances and the actual distances do not match up: the upper and lower points are too close to the center, while the ones in the middle are too far away. We use the target distances in our constraints to learn a distance function and use this function to transform the original space. The result is that the distances in the transformed feature space now align much more closely to the target distances.

%contributions: describes method for metric learning with real-valued constraints, describes evaluation problem and datasets, evaluates described and related methods on real-valued constraints
This thesis describes the metric alignment method in detail and compares it against existing methods for metric learning. We apply our method to images taken from a semantic segmentation problem in order to predict semantic overlap between image patches, and to images from an attribute-based classification problem in order to predict an attribute vector from the image features. We compare our method against existing methods where we threshold the real-valued loss in order to generate similarity/dissimilarity constraints. In summary, the main contributions of this thesis are as follows:
\begin{itemize}
\item We describe a method for metric learning with absolute distance constraints in the context of structured prediction problems.
\item We introduce two learning problems based on existing datasets that can be used to evaluate metric learning methods with absolute distance constraints.
\item We evaluate our method against existing methods on these learning problems.
\end{itemize}
